{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d802096",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827b91a",
   "metadata": {},
   "source": [
    "在深层神经网络训练过程中, 会出现准确率随着网络深度增加(在一个已经达到较好的效果的浅层网络上加入恒等映射层), 先增大后减小, 神经网络会退化. 过深的网络无法拟合线性映射. 因此需要使用残差方法."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbadfcd",
   "metadata": {},
   "source": [
    "在神经网络优化中, 如果在某一层以后需要一个恒等映射$H(x) = x$来拟合下一层的函数, 对于多层网络而言很困难, 于是需要一个残差函数$H(x)  = F(x) + x$来让网络学习$F(x) = H(x) - x$ 来避免直接对恒等映射直接学习, 如果此处函数应该是恒等映射的话, 那么只需要学习到$F(x) == 0$即可. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe90c6",
   "metadata": {},
   "source": [
    "此处网络连接成为shortcut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4483896",
   "metadata": {},
   "source": [
    "![shortcut](shortcut.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702354e",
   "metadata": {},
   "source": [
    "假设shortcut从$m$层进行映射, 假设$m+1$层(可以跨越多层)可能学习为恒等映射, 那么为了防止直接学习恒等映射. 在第$m+2$层加入$m$层的原始数据, 这样, 如果需要拟合为恒等映射, 只需要将$m+1$层函数置为零即可(对于ReLU只需要让其变为负数, 即可变为零)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226c920",
   "metadata": {},
   "source": [
    "![pic](mtom2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80462aad",
   "metadata": {},
   "source": [
    "对于此矩阵表达式为(bias算在$X$内)\n",
    "$$Y^{(m+2)} = ReLU(X^{(m+1)}W_{m+2} + X^{(m)}W_s) \\\\\n",
    " X^{(m+1)} = ReLU(X^{(m)}W_{m+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132429de",
   "metadata": {},
   "source": [
    "$W_s$是变换矩阵, 用于匹配将$m$层矩阵映射到$m+2$层时同型可加. 一般采用全零padding或者使用1\\*1卷积核对其变化维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c872c5c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa75172",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b44ae6",
   "metadata": {},
   "source": [
    "与res类似, 加入shortcut, 但是与之不同的是, 网络的每一层都与后续的网络层数相互连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b91fa",
   "metadata": {},
   "source": [
    "与res不同, densenet两层shortcut时并不是简单的相加, 而是将前层的的特征神经元与后者拼接, 也就是说前者的神经元仍参与本层的运算. 实现特征重用(Feature Reuse). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b714d4",
   "metadata": {},
   "source": [
    "在网络深度加深时, 后续层数输入维度将变得巨大, 为减少计算, 采用1*1卷积降低深度, 从而减少特征维度."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4f90a",
   "metadata": {},
   "source": [
    "![dense](dense.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfafd26",
   "metadata": {},
   "source": [
    "优点是在后续决策中低层的特征也可以直接参与"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d88fe1bbb4da71be0798a46453a5efff5f674a594c343179d37cef3200b399ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
